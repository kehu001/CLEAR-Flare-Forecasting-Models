{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cee19b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ftplib\n",
    "import os\n",
    "import tarfile\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8600766",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file_from_ftp(ftp_server, ftp_path, local_file):\n",
    "    ftp = ftplib.FTP(ftp_server)\n",
    "    ftp.login()\n",
    "    \n",
    "    ftp.cwd(os.path.dirname(ftp_path))\n",
    "    \n",
    "    with open(local_file, \"wb\") as f:\n",
    "        ftp.retrbinary(f\"RETR \" + os.path.basename(ftp_path), f.write)\n",
    "\n",
    "    ftp.quit()\n",
    "    print(f\"Downloaded {local_file}\")\n",
    "\n",
    "def extract_txt_files(tar_file, extract_to_path):\n",
    "    if tarfile.is_tarfile(tar_file):\n",
    "        with tarfile.open(tar_file) as tar:\n",
    "            tar.extractall(path=extract_to_path)\n",
    "            print(f\"Extracted all files to {extract_to_path}\")\n",
    "            \n",
    "            txt_files = [member.name for member in tar.getmembers() if member.isfile() and member.name.endswith(\".txt\")]\n",
    "            return txt_files\n",
    "    else:\n",
    "        print(f\"{tar_file} is not a valid tar.gz file\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9bfcd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pattern1(txt_files,extract_to_path, year):\n",
    "    # for year 2010-2024 \n",
    "    if txt_files:\n",
    "        start_time = []\n",
    "        peak_time = []\n",
    "        end_time = []\n",
    "        cls = []\n",
    "        noaa_ar = []\n",
    "        obs = []\n",
    "        for txt_file in txt_files:\n",
    "            # read txt file\n",
    "            if txt_file == 'readme.txt':\n",
    "                continue\n",
    "            if year == 2025:\n",
    "                path = os.path.join(extract_to_path, '2025_events', txt_file)\n",
    "            else:\n",
    "                path = os.path.join(extract_to_path, txt_file)\n",
    "            with open(path) as f:\n",
    "                data = f.readlines()\n",
    "                # get flare events info\n",
    "                date = data[2][7:-1].replace(' ', '/')\n",
    "                if data[13:][0] == 'NO EVENT REPORTS.\\n' :\n",
    "                    continue\n",
    "                for line in data[13:]:\n",
    "                # skip the new line character\n",
    "                    if line == '\\n':\n",
    "                        continue\n",
    "                    # split the line by space\n",
    "                    line = line.replace('+', '').split()\n",
    "                    if len(line) < 9:\n",
    "                        continue\n",
    "                    # if obs not start with G, skip the line\n",
    "                    if not line[4].startswith('G'):\n",
    "                        continue\n",
    "                    start = date + ' ' + line[1][:2] + ':' + line[1][2:]\n",
    "                    peak = date + ' ' + line[2][:2] + ':' + line[2][2:]\n",
    "                    end = date + ' ' + line[3][:2] + ':' + line[3][2:]\n",
    "                    start_time.append(start)\n",
    "                    peak_time.append(peak)\n",
    "                    end_time.append(end)\n",
    "                    cls.append(line[8])\n",
    "                    obs.append(line[4])\n",
    "                    if len(line) == 11:\n",
    "                        noaa_ar.append('1'+line[-1])\n",
    "                    else:\n",
    "                        noaa_ar.append(0)\n",
    "        # save to csv\n",
    "        df = pd.DataFrame({'start_time': start_time, 'peak_time': peak_time, 'end_time': end_time, 'class': cls, 'noaa_ar': noaa_ar, 'obs': obs})\n",
    "        # reordering by start_time\n",
    "        #df['start_time'] = pd.to_datetime(df['start_time'])\n",
    "        #df = df.sort_values(by='start_time')\n",
    "        df.to_csv(f'ftp_flares_{year}.csv', index=False)\n",
    "        print(f\"Save ftp_flares_{year}.csv\")\n",
    "    \n",
    "def get_ftp_files(year):\n",
    "    # this function get flare events info by year from ftp://ftp.swpc.noaa.gov/pub/warehouse/\n",
    "    # return a df and save a csv file\n",
    "    ftp_server = \"ftp.swpc.noaa.gov\"\n",
    "    ftp_path = f\"/pub/warehouse/{year}/{year}_events.tar.gz\"\n",
    "    local_file = f\"{year}_events.tar.gz\"\n",
    "    extract_to_path = f\"./extracted_txt_files_{year}\"\n",
    "\n",
    "    \n",
    "    #if year == 2024:\n",
    "        #txt_files = download_2024()\n",
    "        #pattern1(txt_files, extract_to_path, year)\n",
    "    if year in [2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009 ,2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024]:\n",
    "        download_file_from_ftp(ftp_server, ftp_path, local_file)\n",
    "        txt_files = extract_txt_files(local_file, extract_to_path)\n",
    "        pattern1(txt_files, extract_to_path, year)\n",
    "    if year in [1996, 1997, 1998, 1999]:\n",
    "        download_file_from_ftp(ftp_server, ftp_path, local_file)\n",
    "        txt_files = extract_txt_files(local_file, extract_to_path)\n",
    "        pattern2(txt_files, extract_to_path, year)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7757e141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 2024_events.tar.gz\n",
      "Extracted all files to ./extracted_txt_files_2024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huke0\\AppData\\Local\\Temp\\ipykernel_17736\\2315682492.py:16: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
      "  tar.extractall(path=extract_to_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save ftp_flares_2024.csv\n"
     ]
    }
   ],
   "source": [
    "get_ftp_files(2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9ca8f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88acf37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "def download_ftp_flares(\n",
    "    ftp_server=\"ftp.swpc.noaa.gov\",\n",
    "    ftp_path=\"/pub/warehouse/2025/2025_events\",\n",
    "    local_dir=\"./extracted_txt_files_2025/2025_events\",\n",
    "    cutoff_date=\"2025-10-31\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Download SWPC event text files newer than cutoff_date (YYYY-MM-DD).\n",
    "    Example file name pattern: 20251031events.txt\n",
    "    \"\"\"\n",
    "    os.makedirs(local_dir, exist_ok=True)\n",
    "    cutoff_dt = datetime.strptime(cutoff_date, \"%Y-%m-%d\")\n",
    "\n",
    "    ftp = ftplib.FTP(ftp_server)\n",
    "    ftp.login()\n",
    "    ftp.cwd(ftp_path)\n",
    "\n",
    "    files = ftp.nlst()\n",
    "    pattern = re.compile(r\"(\\d{8})events\\.txt\", re.IGNORECASE)\n",
    "\n",
    "    downloaded = []\n",
    "    for file in files:\n",
    "        m = pattern.match(file)\n",
    "        if not m:\n",
    "            continue\n",
    "\n",
    "        file_date = datetime.strptime(m.group(1), \"%Y%m%d\")\n",
    "        if file_date <= cutoff_dt:\n",
    "            continue  # skip older files\n",
    "\n",
    "        local_file = os.path.join(local_dir, file)\n",
    "        if os.path.exists(local_file):\n",
    "            print(f\"Skip existing: {file}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Downloading {file} ...\")\n",
    "        with open(local_file, \"wb\") as f:\n",
    "            ftp.retrbinary(f\"RETR {file}\", f.write)\n",
    "        downloaded.append(file)\n",
    "\n",
    "    ftp.quit()\n",
    "    print(f\"Downloaded {len(downloaded)} new files.\")\n",
    "    return True\n",
    "\n",
    "def list_swpc_txt_files(txt_root, year=2025, cutoff_yyyymmdd=\"20251031\"):\n",
    "    \"\"\"\n",
    "    Return a list of txt filenames (not paths) under `txt_root` to parse.\n",
    "    Keeps only files with names like YYYYMMDDevents.txt and later than cutoff.\n",
    "    \"\"\"\n",
    "    txt_root = Path(txt_root)\n",
    "    patt = re.compile(r\"(\\d{8})events\\.txt$\", re.IGNORECASE)\n",
    "\n",
    "    keep = []\n",
    "    for f in sorted(txt_root.glob(\"*.txt\")):\n",
    "        if f.name.lower() == \"readme.txt\":\n",
    "            continue\n",
    "        m = patt.match(f.name)\n",
    "        if not m:\n",
    "            continue\n",
    "        if m.group(1) > cutoff_yyyymmdd:\n",
    "            keep.append(f.name)\n",
    "    return keep\n",
    "\n",
    "def extract_xrs_to_csv(txt_files, extract_to_path, year=2025, out_csv=\"ftp_flares_2025.csv\"):\n",
    "    \"\"\"\n",
    "    Read the given daily 'events' txt files and extract GOES/XRS (flare) rows.\n",
    "    Overwrites `out_csv` on each run.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    txt_files : list[str]\n",
    "        Filenames (not paths) returned by `list_swpc_txt_files(...)`.\n",
    "    extract_to_path : str or Path\n",
    "        Parent folder containing the daily txt files.\n",
    "        For year 2025 we expect files in {extract_to_path}/2025_events/.\n",
    "    year : int\n",
    "        Year of these daily files (used only for building the subfolder name).\n",
    "    out_csv : str\n",
    "        Output CSV path; will be overwritten.\n",
    "    \"\"\"\n",
    "    extract_to_path = Path(extract_to_path)\n",
    "    start_time, peak_time, end_time = [], [], []\n",
    "    cls, noaa_ar, obs = [], [], []\n",
    "\n",
    "    if not txt_files:\n",
    "        # nothing to parse; write empty CSV with headers for reproducibility\n",
    "        pd.DataFrame(columns=[\"start_time\",\"peak_time\",\"end_time\",\"class\",\"noaa_ar\",\"obs\"]).to_csv(out_csv, index=False)\n",
    "        print(f\"Saved (empty) {out_csv}\")\n",
    "        return out_csv\n",
    "\n",
    "    for txt_file in txt_files:\n",
    "        if txt_file.lower() == \"readme.txt\":\n",
    "            continue\n",
    "\n",
    "        # daily file lives in {extract_to_path}/2025_events/ or just extract_to_path/\n",
    "        if year == 2025:\n",
    "            path = extract_to_path / \"2025_events\" / txt_file\n",
    "        else:\n",
    "            path = extract_to_path / txt_file\n",
    "\n",
    "        if not path.exists():\n",
    "            # if the files were saved directly under extract_to_path, try that\n",
    "            alt = extract_to_path / txt_file\n",
    "            if alt.exists():\n",
    "                path = alt\n",
    "            else:\n",
    "                print(f\"Skip missing: {path}\")\n",
    "                continue\n",
    "\n",
    "        with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        if len(lines) < 14:\n",
    "            continue\n",
    "\n",
    "        raw_date = lines[2].strip()\n",
    "        # Extract YYYY.MM.DD (or variants) and normalize to YYYY/MM/DD\n",
    "        date_part = (\n",
    "            raw_date.replace(\"Event Date:\", \"\")\n",
    "                    .replace(\"-\", \".\").replace(\"/\", \".\").replace(\" \", \".\")\n",
    "                    .strip(\".\")\n",
    "        )\n",
    "        parts = [p for p in date_part.split(\".\") if p.isdigit()]\n",
    "        if len(parts) >= 3:\n",
    "            date_str = f\"{parts[0]}/{parts[1]}/{parts[2]}\"  # YYYY/MM/DD\n",
    "        else:\n",
    "            # fallback: skip file if date header is not as expected\n",
    "            continue\n",
    "\n",
    "        # If the table says \"NO EVENT REPORTS.\" then skip\n",
    "        rest = lines[13:]\n",
    "        if not rest or rest[0].strip().upper().startswith(\"NO EVENT REPORTS\"):\n",
    "            continue\n",
    "\n",
    "        for line in rest:\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            # remove '+' signs, split on whitespace\n",
    "            tokens = line.replace(\"+\", \"\").split()\n",
    "            if len(tokens) < 9:\n",
    "                continue\n",
    "\n",
    "            # tokens[4] should be obs, and we only keep GOES rows (starting with 'G')\n",
    "            if not tokens[4].startswith(\"G\"):\n",
    "                continue\n",
    "\n",
    "            # times are HHMM in tokens[1], [2], [3]\n",
    "            try:\n",
    "                s_hhmm = tokens[1]; p_hhmm = tokens[2]; e_hhmm = tokens[3]\n",
    "                s = f\"{date_str} {s_hhmm[:2]}:{s_hhmm[2:]:0>2}\"\n",
    "                p = f\"{date_str} {p_hhmm[:2]}:{p_hhmm[2:]:0>2}\"\n",
    "                e = f\"{date_str} {e_hhmm[:2]}:{e_hhmm[2:]:0>2}\"\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "            start_time.append(s)\n",
    "            peak_time.append(p)\n",
    "            end_time.append(e)\n",
    "\n",
    "            # class at tokens[8]\n",
    "            cls.append(tokens[8])\n",
    "            obs.append(tokens[4])\n",
    "\n",
    "            # NOAA AR present when len == 11 â†’ last token; else 0\n",
    "            ar = tokens[-1] if len(tokens) == 11 and tokens[-1].isdigit() else \"0\"\n",
    "            if ar == \"0\":\n",
    "                noaa_ar.append(\"0\")\n",
    "            else:\n",
    "                noaa_ar.append('1'+ar)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"start_time\": start_time,\n",
    "        \"peak_time\":  peak_time,\n",
    "        \"end_time\":   end_time,\n",
    "        \"class\":      cls,\n",
    "        \"noaa_ar\":    noaa_ar,\n",
    "        \"obs\":        obs\n",
    "    })\n",
    "\n",
    "    # sort by start_time \n",
    "    # df[\"start_time\"] = pd.to_datetime(df[\"start_time\"])\n",
    "    # df = df.sort_values(\"start_time\").reset_index(drop=True)\n",
    "\n",
    "    # Overwrite output CSV on each run\n",
    "    Path(out_csv).parent.mkdir(parents=True, exist_ok=True)\n",
    "    df.to_csv(out_csv, index=False)\n",
    "    print(f\"Saved {out_csv} with {len(df)} XRS flare rows.\")\n",
    "    return out_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9501811",
   "metadata": {},
   "outputs": [],
   "source": [
    "ftp_server=\"ftp.swpc.noaa.gov\"\n",
    "ftp_path=\"/pub/warehouse/2025/2025_events\"\n",
    "local_dir=\"D:\\\\2024_S1\\\\ML_SEP_2402\\\\swpc_ftp\\\\extracted_txt_files_2025\" # Local directory to save downloaded files\n",
    "cutoff_date=\"2024-12-30\" \n",
    "flare_csv = \"D:\\\\2024_S1\\\\ML_SEP_2402\\\\swpc_ftp\\\\ftp_flares_2025.csv\" # Output CSV file for flares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc0d23a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 20250216events.txt ...\n",
      "Downloading 20241231events.txt ...\n",
      "Downloading 20250217events.txt ...\n",
      "Downloading 20250228events.txt ...\n",
      "Downloading 20250218events.txt ...\n",
      "Downloading 20250101events.txt ...\n",
      "Downloading 20250219events.txt ...\n",
      "Downloading 20250102events.txt ...\n",
      "Downloading 20250220events.txt ...\n",
      "Downloading 20250103events.txt ...\n",
      "Downloading 20250221events.txt ...\n",
      "Downloading 20250104events.txt ...\n",
      "Downloading 20250301events.txt ...\n",
      "Downloading 20250105events.txt ...\n",
      "Downloading 20250302events.txt ...\n",
      "Downloading 20250106events.txt ...\n",
      "Downloading 20250303events.txt ...\n",
      "Downloading 20250107events.txt ...\n",
      "Downloading 20250304events.txt ...\n",
      "Downloading 20250108events.txt ...\n",
      "Downloading 20250305events.txt ...\n",
      "Downloading 20250109events.txt ...\n",
      "Downloading 20250306events.txt ...\n",
      "Downloading 20250110events.txt ...\n",
      "Downloading 20250307events.txt ...\n",
      "Downloading 20250111events.txt ...\n",
      "Downloading 20250308events.txt ...\n",
      "Downloading 20250112events.txt ...\n",
      "Downloading 20250309events.txt ...\n",
      "Downloading 20250113events.txt ...\n",
      "Downloading 20250310events.txt ...\n",
      "Downloading 20250114events.txt ...\n",
      "Downloading 20250311events.txt ...\n",
      "Downloading 20250115events.txt ...\n",
      "Downloading 20250312events.txt ...\n",
      "Downloading 20250116events.txt ...\n",
      "Downloading 20250313events.txt ...\n",
      "Downloading 20250117events.txt ...\n",
      "Downloading 20250314events.txt ...\n",
      "Downloading 20250118events.txt ...\n",
      "Downloading 20250315events.txt ...\n",
      "Downloading 20250119events.txt ...\n",
      "Downloading 20250316events.txt ...\n",
      "Downloading 20250120events.txt ...\n",
      "Downloading 20250317events.txt ...\n",
      "Downloading 20250121events.txt ...\n",
      "Downloading 20250318events.txt ...\n",
      "Downloading 20250122events.txt ...\n",
      "Downloading 20250319events.txt ...\n",
      "Downloading 20250123events.txt ...\n",
      "Downloading 20250320events.txt ...\n",
      "Downloading 20250124events.txt ...\n",
      "Downloading 20250321events.txt ...\n",
      "Downloading 20250125events.txt ...\n",
      "Downloading 20250322events.txt ...\n",
      "Downloading 20250126events.txt ...\n",
      "Downloading 20250323events.txt ...\n",
      "Downloading 20250127events.txt ...\n",
      "Downloading 20250324events.txt ...\n",
      "Downloading 20250128events.txt ...\n",
      "Downloading 20250325events.txt ...\n",
      "Downloading 20250129events.txt ...\n",
      "Downloading 20250326events.txt ...\n",
      "Downloading 20250130events.txt ...\n",
      "Downloading 20250327events.txt ...\n",
      "Downloading 20250131events.txt ...\n",
      "Downloading 20250328events.txt ...\n",
      "Downloading 20250201events.txt ...\n",
      "Downloading 20250329events.txt ...\n",
      "Downloading 20250202events.txt ...\n",
      "Downloading 20250330events.txt ...\n",
      "Downloading 20250203events.txt ...\n",
      "Downloading 20250331events.txt ...\n",
      "Downloading 20250204events.txt ...\n",
      "Downloading 20250401events.txt ...\n",
      "Downloading 20250205events.txt ...\n",
      "Downloading 20250402events.txt ...\n",
      "Downloading 20250206events.txt ...\n",
      "Downloading 20250403events.txt ...\n",
      "Downloading 20250207events.txt ...\n",
      "Downloading 20250404events.txt ...\n",
      "Downloading 20250208events.txt ...\n",
      "Downloading 20250405events.txt ...\n",
      "Downloading 20250209events.txt ...\n",
      "Downloading 20250406events.txt ...\n",
      "Downloading 20250210events.txt ...\n",
      "Downloading 20250407events.txt ...\n",
      "Downloading 20250211events.txt ...\n",
      "Downloading 20250408events.txt ...\n",
      "Downloading 20250212events.txt ...\n",
      "Downloading 20250409events.txt ...\n",
      "Downloading 20250213events.txt ...\n",
      "Downloading 20250410events.txt ...\n",
      "Downloading 20250214events.txt ...\n",
      "Downloading 20250411events.txt ...\n",
      "Downloading 20250215events.txt ...\n",
      "Downloading 20250222events.txt ...\n",
      "Downloading 20250412events.txt ...\n",
      "Downloading 20250223events.txt ...\n",
      "Downloading 20250413events.txt ...\n",
      "Downloading 20250224events.txt ...\n",
      "Downloading 20250414events.txt ...\n",
      "Downloading 20250225events.txt ...\n",
      "Downloading 20250415events.txt ...\n",
      "Downloading 20250226events.txt ...\n",
      "Downloading 20250416events.txt ...\n",
      "Downloading 20250227events.txt ...\n",
      "Downloading 20250417events.txt ...\n",
      "Downloading 20250418events.txt ...\n",
      "Downloading 20250419events.txt ...\n",
      "Downloading 20250420events.txt ...\n",
      "Downloading 20250421events.txt ...\n",
      "Downloading 20250422events.txt ...\n",
      "Downloading 20250423events.txt ...\n",
      "Downloading 20250424events.txt ...\n",
      "Downloading 20250425events.txt ...\n",
      "Downloading 20250426events.txt ...\n",
      "Downloading 20250427events.txt ...\n",
      "Downloading 20250428events.txt ...\n",
      "Downloading 20250429events.txt ...\n",
      "Downloading 20250430events.txt ...\n",
      "Downloading 20250501events.txt ...\n",
      "Downloading 20250502events.txt ...\n",
      "Downloading 20250503events.txt ...\n",
      "Downloading 20250504events.txt ...\n",
      "Downloading 20250505events.txt ...\n",
      "Downloading 20250506events.txt ...\n",
      "Downloading 20250507events.txt ...\n",
      "Downloading 20250508events.txt ...\n",
      "Downloading 20250509events.txt ...\n",
      "Downloading 20250510events.txt ...\n",
      "Downloading 20250511events.txt ...\n",
      "Downloading 20250512events.txt ...\n",
      "Downloading 20250513events.txt ...\n",
      "Downloading 20250514events.txt ...\n",
      "Downloading 20250515events.txt ...\n",
      "Downloading 20250516events.txt ...\n",
      "Downloading 20250517events.txt ...\n",
      "Downloading 20250518events.txt ...\n",
      "Downloading 20250519events.txt ...\n",
      "Downloading 20250520events.txt ...\n",
      "Downloading 20250707events.txt ...\n",
      "Downloading 20250521events.txt ...\n",
      "Downloading 20250708events.txt ...\n",
      "Downloading 20250522events.txt ...\n",
      "Downloading 20250709events.txt ...\n",
      "Downloading 20250523events.txt ...\n",
      "Downloading 20250710events.txt ...\n",
      "Downloading 20250524events.txt ...\n",
      "Downloading 20250711events.txt ...\n",
      "Downloading 20250525events.txt ...\n",
      "Downloading 20250712events.txt ...\n",
      "Downloading 20250526events.txt ...\n",
      "Downloading 20250713events.txt ...\n",
      "Downloading 20250527events.txt ...\n",
      "Downloading 20250714events.txt ...\n",
      "Downloading 20250528events.txt ...\n",
      "Downloading 20250715events.txt ...\n",
      "Downloading 20250529events.txt ...\n",
      "Downloading 20250716events.txt ...\n",
      "Downloading 20250530events.txt ...\n",
      "Downloading 20250717events.txt ...\n",
      "Downloading 20250531events.txt ...\n",
      "Downloading 20250718events.txt ...\n",
      "Downloading 20250601events.txt ...\n",
      "Downloading 20250719events.txt ...\n",
      "Downloading 20250602events.txt ...\n",
      "Downloading 20250720events.txt ...\n",
      "Downloading 20250603events.txt ...\n",
      "Downloading 20250721events.txt ...\n",
      "Downloading 20250604events.txt ...\n",
      "Downloading 20250722events.txt ...\n",
      "Downloading 20250605events.txt ...\n",
      "Downloading 20250723events.txt ...\n",
      "Downloading 20250606events.txt ...\n",
      "Downloading 20250724events.txt ...\n",
      "Downloading 20250607events.txt ...\n",
      "Downloading 20250725events.txt ...\n",
      "Downloading 20250608events.txt ...\n",
      "Downloading 20250726events.txt ...\n",
      "Downloading 20250609events.txt ...\n",
      "Downloading 20250727events.txt ...\n",
      "Downloading 20250610events.txt ...\n",
      "Downloading 20250728events.txt ...\n",
      "Downloading 20250611events.txt ...\n",
      "Downloading 20250729events.txt ...\n",
      "Downloading 20250612events.txt ...\n",
      "Downloading 20250730events.txt ...\n",
      "Downloading 20250613events.txt ...\n",
      "Downloading 20250731events.txt ...\n",
      "Downloading 20250614events.txt ...\n",
      "Downloading 20250801events.txt ...\n",
      "Downloading 20250615events.txt ...\n",
      "Downloading 20250802events.txt ...\n",
      "Downloading 20250616events.txt ...\n",
      "Downloading 20250803events.txt ...\n",
      "Downloading 20250617events.txt ...\n",
      "Downloading 20250804events.txt ...\n",
      "Downloading 20250618events.txt ...\n",
      "Downloading 20250805events.txt ...\n",
      "Downloading 20250619events.txt ...\n",
      "Downloading 20250806events.txt ...\n",
      "Downloading 20250620events.txt ...\n",
      "Downloading 20250807events.txt ...\n",
      "Downloading 20250621events.txt ...\n",
      "Downloading 20250808events.txt ...\n",
      "Downloading 20250622events.txt ...\n",
      "Downloading 20250809events.txt ...\n",
      "Downloading 20250623events.txt ...\n",
      "Downloading 20250810events.txt ...\n",
      "Downloading 20250624events.txt ...\n",
      "Downloading 20250811events.txt ...\n",
      "Downloading 20250625events.txt ...\n",
      "Downloading 20250812events.txt ...\n",
      "Downloading 20250626events.txt ...\n",
      "Downloading 20250813events.txt ...\n",
      "Downloading 20250627events.txt ...\n",
      "Downloading 20250814events.txt ...\n",
      "Downloading 20250628events.txt ...\n",
      "Downloading 20250815events.txt ...\n",
      "Downloading 20250629events.txt ...\n",
      "Downloading 20250816events.txt ...\n",
      "Downloading 20250630events.txt ...\n",
      "Downloading 20250817events.txt ...\n",
      "Downloading 20250701events.txt ...\n",
      "Downloading 20250818events.txt ...\n",
      "Downloading 20250702events.txt ...\n",
      "Downloading 20250819events.txt ...\n",
      "Downloading 20250703events.txt ...\n",
      "Downloading 20250820events.txt ...\n",
      "Downloading 20250704events.txt ...\n",
      "Downloading 20250821events.txt ...\n",
      "Downloading 20250705events.txt ...\n",
      "Downloading 20250822events.txt ...\n",
      "Downloading 20250706events.txt ...\n",
      "Downloading 20250823events.txt ...\n",
      "Downloading 20250824events.txt ...\n",
      "Downloading 20250825events.txt ...\n",
      "Downloading 20250826events.txt ...\n",
      "Downloading 20250827events.txt ...\n",
      "Downloading 20250828events.txt ...\n",
      "Downloading 20250829events.txt ...\n",
      "Downloading 20250830events.txt ...\n",
      "Downloading 20250831events.txt ...\n",
      "Downloading 20250901events.txt ...\n",
      "Downloading 20250902events.txt ...\n",
      "Downloading 20250903events.txt ...\n",
      "Downloading 20250904events.txt ...\n",
      "Downloading 20250905events.txt ...\n",
      "Downloading 20250906events.txt ...\n",
      "Downloading 20250907events.txt ...\n",
      "Downloading 20250908events.txt ...\n",
      "Downloading 20250909events.txt ...\n",
      "Downloading 20250910events.txt ...\n",
      "Downloading 20250911events.txt ...\n",
      "Downloading 20250912events.txt ...\n",
      "Downloading 20250913events.txt ...\n",
      "Downloading 20250914events.txt ...\n",
      "Downloading 20250915events.txt ...\n",
      "Downloading 20250916events.txt ...\n",
      "Downloading 20250917events.txt ...\n",
      "Downloading 20250918events.txt ...\n",
      "Downloading 20250919events.txt ...\n",
      "Downloading 20250920events.txt ...\n",
      "Downloading 20250921events.txt ...\n",
      "Downloading 20250922events.txt ...\n",
      "Downloading 20250923events.txt ...\n",
      "Downloading 20250924events.txt ...\n",
      "Downloading 20250925events.txt ...\n",
      "Downloading 20250926events.txt ...\n",
      "Downloading 20250927events.txt ...\n",
      "Downloading 20250928events.txt ...\n",
      "Downloading 20250929events.txt ...\n",
      "Downloading 20250930events.txt ...\n",
      "Downloading 20251001events.txt ...\n",
      "Downloading 20251002events.txt ...\n",
      "Downloading 20251003events.txt ...\n",
      "Downloading 20251004events.txt ...\n",
      "Downloading 20251005events.txt ...\n",
      "Downloading 20251006events.txt ...\n",
      "Downloading 20251007events.txt ...\n",
      "Downloading 20251008events.txt ...\n",
      "Downloading 20251009events.txt ...\n",
      "Downloading 20251010events.txt ...\n",
      "Downloading 20251011events.txt ...\n",
      "Downloading 20251012events.txt ...\n",
      "Downloading 20251013events.txt ...\n",
      "Downloading 20251014events.txt ...\n",
      "Downloading 20251015events.txt ...\n",
      "Downloading 20251016events.txt ...\n",
      "Downloading 20251017events.txt ...\n",
      "Downloading 20251018events.txt ...\n",
      "Downloading 20251019events.txt ...\n",
      "Downloading 20251020events.txt ...\n",
      "Downloading 20251021events.txt ...\n",
      "Downloading 20251022events.txt ...\n",
      "Downloading 20251023events.txt ...\n",
      "Downloading 20251024events.txt ...\n",
      "Downloading 20251025events.txt ...\n",
      "Downloading 20251026events.txt ...\n",
      "Downloading 20251027events.txt ...\n",
      "Downloading 20251028events.txt ...\n",
      "Downloading 20251029events.txt ...\n",
      "Downloading 20251030events.txt ...\n",
      "Downloading 20251031events.txt ...\n",
      "Downloading 20251101events.txt ...\n",
      "Downloading 20251102events.txt ...\n",
      "Downloading 20251103events.txt ...\n",
      "Downloading 20251104events.txt ...\n",
      "Downloading 20251105events.txt ...\n",
      "Downloading 20251106events.txt ...\n",
      "Downloading 20251107events.txt ...\n",
      "Downloading 20251108events.txt ...\n",
      "Downloading 20251109events.txt ...\n",
      "Downloading 20251110events.txt ...\n",
      "Downloading 20251111events.txt ...\n",
      "Downloading 20251112events.txt ...\n",
      "Downloading 20251113events.txt ...\n",
      "Downloading 20251114events.txt ...\n",
      "Downloading 20251115events.txt ...\n",
      "Downloaded 320 new files.\n",
      "Saved D:\\2024_S1\\ML_SEP_2402\\swpc_ftp\\ftp_flares_2025.csv with 2822 XRS flare rows.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'D:\\\\2024_S1\\\\ML_SEP_2402\\\\swpc_ftp\\\\ftp_flares_2025.csv'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "download_ftp_flares(ftp_server, ftp_path, local_dir, cutoff_date)\n",
    "files_to_parse = list_swpc_txt_files(local_dir, year=2025, cutoff_yyyymmdd=re.sub(r\"-\", \"\", cutoff_date))\n",
    "extract_xrs_to_csv(files_to_parse, extract_to_path=local_dir, year=2025, out_csv=flare_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba587985",
   "metadata": {},
   "outputs": [],
   "source": [
    "ftp24 = pd.read_csv(\"ftp_flares_2024.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1da494c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ftp24['noaa_ar'] = ftp24['noaa_ar'].apply(lambda x: \"1\"+str(x) if str(x) != '0' else '0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5786f337",
   "metadata": {},
   "outputs": [],
   "source": [
    "ftp25 = pd.read_csv(\"D:\\\\2024_S1\\\\ML_SEP_2402\\\\swpc_ftp\\\\ftp_flares_2025.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9fe33c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "ftp_updated = pd.concat([ftp24, ftp25], ignore_index=True)\n",
    "ftp_updated.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "82a7e992",
   "metadata": {},
   "outputs": [],
   "source": [
    "ftp_updated.rename(columns={'class': 'label'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fd401996",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls = ftp_updated['label'].apply(lambda x: str(x)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "91e7bafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ftp_updated['cls'] = cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f7341e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "ftp_old = pd.read_csv(\"D:\\\\2024_S1\\\\ML_SEP_2402\\\\swpc_ftp\\\\v2_ftp_flares_1997_2024.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8e6bd978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep 2010-2023\n",
    "ftp_old = ftp_old[ftp_old['start_time'].str.startswith(('2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021', '2022', '2023'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1cd13d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "ftp_old = ftp_old.drop(columns=['unique_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "828f19ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "ftp_new = pd.concat([ftp_old, ftp_updated], ignore_index=True)\n",
    "ftp_new.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "94eb4ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_time    2024/06/01 A1:903\n",
      "peak_time      2024/06/01 19:39\n",
      "end_time      2024/06/01 A2:028\n",
      "label                      M7.3\n",
      "noaa_ar                   13697\n",
      "obs                         G16\n",
      "cls                           M\n",
      "Name: 25413, dtype: object\n",
      "start_time    2024/10/21 B0:840\n",
      "peak_time     2024/10/21 U1:001\n",
      "end_time      2024/10/21 B1:139\n",
      "label                      C6.2\n",
      "noaa_ar                       0\n",
      "obs                         G16\n",
      "cls                           C\n",
      "Name: 26823, dtype: object\n",
      "start_time    2024/11/15 B0:138\n",
      "peak_time     2024/11/15 U0:146\n",
      "end_time      2024/11/15 B0:208\n",
      "label                      M1.1\n",
      "noaa_ar                   13893\n",
      "obs                         G16\n",
      "cls                           M\n",
      "Name: 27055, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# reordering by start_time\n",
    "remove = []\n",
    "for i in range(ftp_new.shape[0]):\n",
    "    try:\n",
    "        pd.to_datetime(ftp_new['start_time'][i])\n",
    "    except:\n",
    "        remove.append(i)\n",
    "        print(ftp_new.iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "786dfe56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the rows with invalid start_time\n",
    "ftp_new = ftp_new.drop(remove, axis=0)\n",
    "ftp_new = ftp_new.sort_values(by='start_time').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "159ac3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the ftp rows where invalid class\n",
    "remove = []\n",
    "for i in range(ftp_new['label'].shape[0]):\n",
    "    if ftp_new['label'].iloc[i][0] not in ['X', 'M', 'C','B','A']:\n",
    "        remove.append(i)\n",
    "        print(ftp_new['label'].iloc[i], ftp_new['start_time'].iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0baee71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ftp_new.to_csv(\"ftp_flares_20100101_20251115.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch126",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
